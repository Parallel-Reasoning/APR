defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

# Data configuration
data:
  train_data_path: data/apr_train_beam10_subbeam15_prefix.json
  val_data_path: data/apr_val_prefix.json
  train_batch_size: 64
  eval_batch_size: 128

# Model configuration
model:
  name: "Parallel-Reasoning/llama-apr-beam10_subbeam15_num_subcall_cond"
  server_url: "http://localhost:0"
  master_port: 0
  mem_fraction_static: 0.6
  gradient_checkpointing: true  # Enable gradient checkpointing to save memory

# Rollout configuration
rollout:
  mode: "apr"
  sample_temperature: 1.0
  eval_temperature: 1.0
  group_size: 5
  # condition_prefix: null
  condition_prefix:  "Sub Call Budget: 10 "

# Training configuration
training:
  learning_rate: 1e-5
  ppo_clip_ratio: 0.2
  num_steps: 150
  inner_steps: 2
  kl_beta: 0.001
  grad_clip: 1.0
  # This grad_accum_chunk_size is used for gradient accumulation
  # It is used to avoid OOM when the batch size is too large
  grad_accum_chunk_size: 4
  log_probs_chunk_size: 16
  validate_per_steps: 50
  # Number of validation samples to use (null for full validation set), set to 16 for testing, set to a larger number for actual validation
  val_samples: 16
  output_dir: checkpoints/apr
  save_steps: 10

# Logging configuration
logging:
  verbose: false
  wandb_project: "tinyrl-apr"
  wandb_name: "apr_cond10_grpo"
  log_interval: 1
  use_current_time: true

hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num} 